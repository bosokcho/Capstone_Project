# Checkpoint 7

** Madeline Abbott, Aidan Teppema, Daisy Cho **

\


```{r include=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(zoo)
library(lubridate)
library(ggmap)
library(reshape2)
library(MASS)
library(viridis)
#library(shiny)
library(rjags)
library(MacBayes)
```

Loading the data...
```{r include=FALSE}
# Loading the Data
hurricane_data <- read_csv("Historical_Tropical_Storm_Tracks.csv")
hurricane_data<-na.omit(hurricane_data)
hurricane_data<-subset(hurricane_data, select=-c(BTID, AD_TIME))

# Recent Hurricanes--select only hurricanes in 1950 and after
recent_hurricanes <- hurricane_data[hurricane_data$YEAR >= 1950, ] 
recent_hurricanes <- recent_hurricanes[!(recent_hurricanes$NAME=="NOTNAMED"),]
#Format Coordinates
dat<-as.character(recent_hurricanes$LAT)
new<-substr(dat,1,nchar(dat))
lat<-as.numeric(new)
dat<-as.character(recent_hurricanes$LONG)
new<-substr(dat,1,nchar(dat))
long<-as.numeric(new)
long<- -abs(long)
coord<-as.data.frame(long)
#Add year and name
year<-as.character(recent_hurricanes$YEAR)
name<-as.character(recent_hurricanes$NAME)
hurricane_locations<-cbind(year, name, coord, lat)

# Hurricane Frequency
# table of hurricanes per year
hurricanes_per_year <- recent_hurricanes %>%
  group_by(YEAR) %>%
  summarise(TOTAL_H = n_distinct(NAME))
hurricanes_per_basin <- recent_hurricanes %>%
  group_by(BASIN) %>%
  summarise(TOTAL_H = n_distinct(NAME, DAY))

# Temperature Data
temp_data <- read.csv("ZonAnn.csv")
temp_data[!complete.cases(temp_data),]
temp_data<-na.omit(temp_data)
temp_data<-temp_data[,-(5:15),drop=FALSE]
temp_recent <- temp_data %>%
  filter(Year > 1949)
# select only northern hemisphere temp data
tempN <- subset(temp_recent, select = c(Year, NHem))
# rename the columns
colnames(tempN) <- c("YEAR", "temp")
# join temperature to hurricanes per year table
hurricanes_per_year2 <- merge(x = hurricanes_per_year, y = tempN, by = "YEAR", all.x = TRUE)

# CO2 Data
CO2 <- read_csv("CO2.csv")
# Alter CO2 to remove unnecessary variable trend
CO2 <- subset(CO2, select = -trend)
# get average by year
CO2_yr <- CO2 %>%
  group_by(year) %>%
  summarise(avg_CO2 = mean(average))
# rename columns
colnames(CO2_yr) <- c("YEAR", "avg_CO2")
# join with hurricane per year table
hurricanes_per_year3 <- merge(x = hurricanes_per_year2, y = CO2_yr, by = "YEAR", all.x = TRUE)
# FIX THIS (bc only 1980 onward available)

# NAO Index Data
NAO_index1 <- read_csv("NAO_index_monthly.csv", 
    col_types = cols(Apr = col_number(), 
        Aug = col_number(), Dec = col_number(), 
        Feb = col_number(), Jan = col_number(), 
        Jul = col_number(), Jun = col_number(), 
        Mar = col_number(), May = col_number(), 
        Nov = col_number(), Oct = col_number(), 
        Sep = col_number()))
NAO_index2 <- NAO_index1[, -c(14)]
NAO_index3 <- NAO_index2 %>%
  rename(year = X1) %>%
  mutate(Jul = replace(Jul, Jul < -50| Jul > 50, "")) %>%
  mutate(Aug = replace(Aug, Aug < -50| Aug > 50, "")) %>%
  mutate(Sep = replace(Sep, Sep < -50| Sep > 50, "")) %>%
  mutate(Oct = replace(Oct, Oct < -50| Oct > 50, "")) %>%
  mutate(Nov = replace(Nov, Nov < -50| Nov > 50, "")) %>%
  mutate(Dec = replace(Dec, Dec < -50| Dec > 50, ""))
NAO_index <- NAO_index3[-nrow(NAO_index3),] 
# remove 2017 (data for year is incomplete)
NAO_index <- head(NAO_index, -1) 
# reshape table
NAO_index <- melt(NAO_index, id=c("year"))
colnames(NAO_index)[2] <- "month"
colnames(NAO_index)[3] <- "index"
NAO_index$year <- as.factor(NAO_index$year)
NAO_index$index <- as.numeric(NAO_index$index)
# group by year
NAO_yr <- NAO_index %>%
  group_by(year) %>%
  summarise(avg_NAO = mean(index))
# rename columns
colnames(NAO_yr) <- c("YEAR", "avg_NAO")
# join with hurricane data
hurricanes_per_year4 <- merge(x = hurricanes_per_year3, y = NAO_yr, by = "YEAR", all.x = TRUE)

# Remove all rows with missing data
hurricanes_per_year_clean<-na.omit(hurricanes_per_year4)

head(hurricanes_per_year_clean)
```


# Progress Made:

Possible things we could do:

* include only Atlantic ocean hurricanes [ ]

* check for colinearity between explanatory variables [DONE]

* include year as a predictor in model [ ]

* model for hurricane severity (see Alicia's email) [ ]

* predict occurance of next (severe landfall?) hurricane using exponential distribution [ ]

\
\


# Group Member Roles:

* Madeline--
* Daisy--
* Aidan--

\
\


# Modeling:


\


# Creating Testing and Training Data
```{r}
# use hurricanes per year data
rownames(hurricanes_per_year_clean) <- 1:nrow(hurricanes_per_year_clean)

# select first 80% of data
train_ind <- rep(1:(0.80*nrow(hurricanes_per_year_clean)))
hurricanes_per_year_train <- hurricanes_per_year_clean[train_ind, ]

# use the remaining 20% of the data for testing
hurricanes_per_year_test <- hurricanes_per_year_clean[-train_ind, ]
```


## Question 1.

Can we model the change in frequency of hurricanes based on trends in hurricane windspeed and category, temperature, CO2, and NAO index?


First, we check for colinarity between the explanatory variables of temperature, CO2, and NAO index.
```{r}
cor(x = as.matrix(hurricanes_per_year_clean$temp), y = as.matrix(hurricanes_per_year_clean$avg_CO2))

cor(x = as.matrix(hurricanes_per_year_clean$avg_NAO), y = as.matrix(hurricanes_per_year_clean$temp))

cor(x = as.matrix(hurricanes_per_year_clean$avg_NAO), y = as.matrix(hurricanes_per_year_clean$avg_CO2))
```

Based on the correlation coefficient between temperature and average CO2 concentration, we remove temperature to reduce collinarity.

Build a new poisson regression model for predicting using only NAO index and CO2 concentration.

Let:

$Y_{t} =$ total hurricanes in year $t$

$X_{1t} =$ yearly CO2 concentration in year $t$

$X_{2t} =$ NAO index in year $t$

We propose the following model:

$$Y_t | \beta_{0}, \beta_{1}, \beta_{2} \sim Pois(\lambda_i)$$

$$log(\lambda_i) = \beta_0 + \beta_{1}X_{1t} + \beta_{2}X_{2t}$$

$$\beta_0 \sim N(0, 1e6)$$

$$\beta_{1} \sim N(0, 1e6)$$

$$\beta_{2} \sim N(0, 1e6)$$



Simulating the model using rjags:
<<<<<<< HEAD
```{r}
head(hurricanes_per_year_train)
=======
```{r echo=FALSE}
head(hurricanes_per_year_clean)
>>>>>>> 9253d0d5ee157444cd7fe9184841d0ec9949ff8c

hur_mod <- " model {
  for (i in 1:length(TOTAL_H)) {
      TOTAL_H[i] ~ dpois(lam[i])
      log(lam[i]) = beta0 + beta1*X1[i] + beta2*X2[i]
  }

  beta0 ~ dnorm(0.0, 1.0/1e6)
  beta1 ~ dnorm(0.0, 1.0/1e4)
  beta2 ~ dnorm(0.0, 1.0/1e4)
} "

# set up an algorithm to simulate the posterior by combining the model and data (x)
# set the random number seed

data_jags = as.list(hurricanes_per_year_clean[,2:5])
str(data_jags)

# manually initialize parameters
freq_jags <- jags.model(textConnection(hur_mod),data=list(TOTAL_H = hurricanes_per_year_train$TOTAL_H, X1 = hurricanes_per_year_train$avg_CO2,X2 = hurricanes_per_year_train$avg_NAO), inits=list(beta0=5,beta1=0, beta2=0))

# simulate a sample from the posterior
# note that we specify both mu and tau variables
freq_sim <- coda.samples(freq_jags, variable.names = c("beta0", "beta1", "beta2"), n.iter=5000)

# store the samples in a data frame:
freq_sample <- data.frame(step = 1:5000, freq_sim[[1]])
head(freq_sample, 10)

plot(freq_sim)


# make a dataframe of parameter values every other step to make plotting faster
Nth.delete<-function(dataframe, n)dataframe[-(seq(n,to=nrow(dataframe),by=n)),]
freq_sample_small <- Nth.delete(freq_sample, 2)
```

Looking at convergence
```{r echo=FALSE}
library(MacBayes)
#running_mean_plot(x=freq_sample_small$beta0, se=TRUE)
#running_mean_plot(x=freq_sample_small$beta1, se=TRUE)
#running_mean_plot(x=freq_sample_small$beta2, se=TRUE)
```

Based on the running mean plots, parameters look like they have converged.  Parameter mean values and 95% credible intervals are as follows:
```{r echo=FALSE}
quantile(freq_sample$beta0, c(0.05, 0.975)) 
quantile(freq_sample$beta1, c(0.05, 0.975))
quantile(freq_sample$beta2, c(0.05, 0.975))

mean(freq_sample$beta0)
mean(freq_sample$beta1)
mean(freq_sample$beta2)
```

Beta0 has a mean of 3.543205 and a 95% CI of (1.209219, 6.417573.

Beta1 has a mean of -0.000656804 and a 95% CI of (-0.007524760,  0.007268657).

Beta2 has a mean of 0.03147051 and a 95% CI of (-0.0852778,  0.1710288).

\

From these results, we conclude that average annual CO2 concentration and NAO index are not useful predictors of hurricane frequency.
FIX THIS
```{r}
#log(lam[i]) = beta0 + beta1*X1[i] + beta2*X2[i]

predictions <- rep(0, nrow(hurricanes_per_year_test)) 
for (i in 1:nrow(hurricanes_per_year_test)){
  predictions[i] <- rpois(1, lambda=exp(freq_sample$beta0 + freq_sample$beta1*hurricanes_per_year_test$avg_CO2[i] + freq_sample$beta2*hurricanes_per_year_test$avg_NAO[i]))
}
```





## Hurricanes by Zones

Divide northern hemisphere into four zones.  Get temperature from 1950 for each of these zones.
```{r}
temp_data <- read_csv("ZonAnn.csv")
temp_data<-na.omit(temp_data) %>%
  filter(1950 <= Year) %>%
  filter(Year <= 2008)
my_temp <- subset(temp_data, select = c("Year", "64N-90N", "44N-64N", "24N-44N", "EQU-24N"))
colnames(my_temp) <- c("Year", "N64to90", "N44to64", "N24to44", "EQUtoN24")
my_temp <- melt(my_temp, id=c("Year"))
colnames(my_temp) <- c("year", "zone", "temp")
head(my_temp)
```

Now divide hurricane observations into these same four zones.
(Currently using all hurricane locations to see if temperature affects where hurricanes occur, not just where they are the most severe.  Could also switch to use just the location at which each hurricane is most severe.)
```{r}
head(hurricane_locations)


hurricane_locations_Atlantic <- merge(hurricane_locations, recent_hurricanes, by.x = c("name", "year", "lat", "long"), by.y = c("NAME", "YEAR", "LAT", "LONG"), all.x = TRUE) %>%
  filter(BASIN == "North Atlantic") %>%
  dplyr::select(c(year, name, long, lat))

hurricanes_zones <- hurricane_locations_Atlantic %>% 
  mutate(zone=cut(lat, breaks=c(0, 24, 44, 64, 90), labels=c("EQUtoN24", "N24to44", "N44to64", "N64to90"))) %>%
  group_by(year, zone)  %>%
  summarise(total = n())

head(hurricanes_zones, 40)

```

Combine the hurricane and temperature data frames together
```{r}
hurricanes_per_zone <- merge(my_temp, hurricanes_zones, by = c("year", "zone"), all.x = TRUE, all.y = TRUE)
hurricanes_per_zone[is.na(hurricanes_per_zone)] <- 0
head(hurricanes_per_zone)
```


Poisson regression--just with temperature
```{r}
hur_mod2 <- " model {
  for (i in 1:length(total)) {
      total[i] ~ dpois(lam[i])
      log(lam[i]) = beta0 + beta1*X1[i]
  }

  beta0 ~ dnorm(0.0, 1.0/1e6)
  beta1 ~ dnorm(0.0, 1.0/1e4)
} "

# set up an algorithm to simulate the posterior by combining the model and data (x)
# set the random number seed
freq_jags2 <- jags.model(textConnection(hur_mod2),data=list(total = hurricanes_per_zone$total, X1 = hurricanes_per_zone$temp), inits=list(.RNG.name="base::Wichmann-Hill", .RNG.seed=2000))

# simulate a sample from the posterior
freq_sim2 <- coda.samples(freq_jags2, variable.names = c("beta0", "beta1"), n.iter=500000)

# store the samples in a data frame:
freq_sample2 <- data.frame(step = 1:500000, freq_sim2[[1]])
head(freq_sample2, 10)
```

Look at the estimated parameters--have they coverged?
```{r}
running_mean_plot(x=freq_sample2$beta0, se=TRUE)
running_mean_plot(x=freq_sample2$beta1, se=TRUE)
```

Based on the running mean plots, parameters look like they have converged.  Parameter mean values and 95% credible intervals are as follows:
```{r}
quantile(freq_sample2$beta0, c(0.05, 0.975)) 
quantile(freq_sample2$beta1, c(0.05, 0.975))

mean(freq_sample2$beta0)
mean(freq_sample2$beta1)
```

beta1, x1 is temp; beta2, x2 is zone

Look at the data
```{r}
head(hurricanes_per_zone)
train_zone <- hurricanes_per_zone %>%
  filter(year < 1996)
test_zone <- hurricanes_per_zone %>%
  filter(year >= 1996)
```



Poisson regression--with temperature and location zone
```{r fig.height=8}
hur_mod3 <- " model {
  for (i in 1:length(total)) {
      total[i] ~ dpois(lam[i])
      log(lam[i]) = beta0 + beta1*X1[i] + beta2[X2[i]]
  }

  beta0 ~ dnorm(0.0, 1.0/1e4)
  beta1 ~ dnorm(0.0, 1.0/1e4)
  beta2[1] <- 0
  for (i in 2:4) {
    beta2[i] ~ dnorm(0.0, 1.0/1e4)
  }
} "

# set up an algorithm to simulate the posterior by combining the model and data (x)
# set the random number seed
#freq_jags3 <- jags.model(textConnection(hur_mod3),data=list(total = train_zone$total, X1 = train_zone$temp, X2 = train_zone$zone), inits=list(.RNG.name="base::Wichmann-Hill", .RNG.seed=2000))

# try to manuall initialize params
freq_jags3 <- jags.model(textConnection(hur_mod3),data=list(total = train_zone$total, X1 = train_zone$temp, X2 = train_zone$zone), inits=list(beta0=0, beta1=0, beta2 = c(NA, 8, 10, 10)))


# simulate a sample from the posterior
<<<<<<< HEAD
freq_sim3 <- coda.samples(freq_jags3, variable.names = c("beta0", "beta1", "beta2"), n.iter=2500000)
=======
freq_sim3 <- coda.samples(freq_jags3, variable.names = c("beta1", "beta2"), n.iter=1000)
>>>>>>> 9253d0d5ee157444cd7fe9184841d0ec9949ff8c

# store the samples in a data frame:
freq_sample3 <- data.frame(step = 1:2500000, freq_sim3[[1]])
head(freq_sample3, 10)
plot(freq_sim3)
<<<<<<< HEAD

freq_sample3_small <- Nth.delete(freq_sample3, 2)

plot_freq_sample3 <- freq_sample3 %>%
  filter(step > (2500000-100000))
=======
>>>>>>> 9253d0d5ee157444cd7fe9184841d0ec9949ff8c
```



Check for convergence
```{r}
running_mean_plot(x=freq_sample3_small$beta0, se=TRUE)
running_mean_plot(x=freq_sample3_small$beta1, se=TRUE)

running_mean_plot(x=freq_sample3_small$beta2.1, se=TRUE)
running_mean_plot(x=freq_sample3_small$beta2.2., se=TRUE)
running_mean_plot(x=freq_sample3_small$beta2.3., se=TRUE)
running_mean_plot(x=freq_sample3_small$beta2.4., se=TRUE)



running_mean_plot(x=plot_freq_sample3$beta2.1., se=TRUE)
running_mean_plot(x=plot_freq_sample3$beta2.2., se=TRUE)
running_mean_plot(x=plot_freq_sample3$beta2.3., se=TRUE)
running_mean_plot(x=plot_freq_sample3$beta2.4., se=TRUE)

```



Look at parameter values
```{r}
# beta 0
quantile(freq_sample3$beta0, c(0.025, 0.975))
# beta 1
quantile(freq_sample3$beta1, c(0.025, 0.975))
# beta 2.2
quantile(freq_sample3$beta2.2., c(0.025, 0.975))
# beta 2.3
quantile(freq_sample3$beta2.3., c(0.025, 0.975))
# beta 2.4
quantile(freq_sample3$beta2.4., c(0.025, 0.975))

# beta 0
mean(freq_sample3$beta0)
# beta 1
mean(freq_sample3$beta1)
# beta 2.2
mean(freq_sample3$beta2.2.)
# beta 2.3
mean(freq_sample3$beta2.3.)
# beta 2.4
mean(freq_sample3$beta2.4.)
<<<<<<< HEAD
=======
```




```{r}

# Hurricane data only looking at Date and name
hurricane_time<-subset(hurricanes_recent, select = c(NAME,YEAR,MONTH,DAY))

#Group data by year and name and subtracting starting time from end time for each hurricane (duration)
foo <- hurricane_time %>%
  group_by(YEAR, NAME) %>%
  mutate(date = as.Date(paste(YEAR, MONTH, DAY, sep='-')), "%Y-%m-%d") %>%
  summarise(duration = max(date) - min(date))

#removed outlier
foo<-foo[!(foo$YEAR==1954 & foo$NAME=="ALICE"),]

# Averaging duration per year
foo<-group_by(foo, YEAR)%>%
  summarise(mean(duration))
# Name second column of data as Average_Duration
colnames(foo)[2] <- "duration"
colnames(foo)[1]<-"year"
#create durations with temperature data, CO2 data, NAOI data and frequency data
durations <- merge(foo, hurricanes_per_zone, by = c("year"), all.x = TRUE, all.y = TRUE)

```


```{r fig.height=6}
library(rjags)

#specify the model
duration_model <- "model{
    #Data
    for(i in 1:length(y)) {
        y[i] ~ dnorm(beta0 + beta1*X1[i] + beta2[X2[i]],tau)
    }

    #Priors
    beta0 ~ dnorm(0.0, 1.0/1e4)
    beta1 ~ dnorm(0.0, 1.0/1e4)
    beta2[1] <- 0
    for (i in 2:4) {
      beta2[i] ~ dnorm(0.0, 1.0/1e4)
  }
    tau ~ dgamma(.001, .001)
}"

>>>>>>> 9253d0d5ee157444cd7fe9184841d0ec9949ff8c

summary(freq_sample3)

<<<<<<< HEAD
```


Testing the model
```{r}
library(dplyr)

# Testing Zone N64to90--begin with the northern-most zone
# select data in this zone
test_zoneN64to90 <- test_zone %>%
  dplyr::filter(c(zone == "N64to90")) %>%
  dplyr::select(-c(zone))
# create prediction function--takes 1000 draws from Poisson distribution
hurri_prediction_zoneN64to90 <- function(X1){
  pred <- rpois(1000, exp(freq_sample3$beta0 + freq_sample3$beta1*X1))
  return(pred)
}
# create empty vector to store predictions
hurri_preds_zoneN64to90 <- rep(0, nrow(test_zoneN64to90))
# get predictions--mean predicted value for each year in this zone
for (i in 1:nrow(test_zoneN64to90)){
  hurri_preds_zoneN64to90[i] <- mean(hurri_prediction_zoneN64to90(X1 = test_zoneN64to90$temp[i]))
}


# Testing Zone N44to64--next northern-most zone
# select data in this zone
test_zoneN44to64 <- test_zone %>%
  dplyr::filter(c(zone == "N44to64")) %>%
  dplyr::select(-c(zone))
# create prediction function--takes 1000 draws from poisson distribution
hurri_prediction_zoneN44to64 <- function(X1){
  pred <- rpois(1000, exp(freq_sample3$beta0 + freq_sample3$beta2.2. + freq_sample3$beta1*X1))
  return(pred)
}
# create empty vector to store predictions
hurri_preds_zoneN44to64 <- rep(0, nrow(test_zoneN44to64))
# get predictions--mean predicted value for each year in this zone
for (i in 1:nrow(test_zoneN44to64)){
  hurri_preds_zoneN44to64[i] <- mean(hurri_prediction_zoneN44to64(X1 = test_zoneN44to64$temp[i]))
}


# Testing Zone N24to44--the next northern-most zone
# select data in this zone
test_zoneN24to44 <- test_zone %>%
  dplyr::filter(c(zone == "N24to44")) %>%
  dplyr::select(-c(zone))
# create prediction function--takes 1000 draws from poisson distribution
hurri_prediction_zoneN24to44 <- function(X1){
  pred <- rpois(1000, exp(freq_sample3$beta0 + freq_sample3$beta2.3. + freq_sample3$beta1*X1))
  return(pred)
}
# create empty vector to store predictions
hurri_preds_zoneN24to44 <- rep(0, nrow(test_zoneN24to44))
# get predictions--mean predicted value for each year in this zone
for (i in 1:nrow(test_zoneN24to44)){
  hurri_preds_zoneN24to44[i] <- mean(hurri_prediction_zoneN24to44(X1 = test_zoneN24to44$temp[i]))
}


# Testing Zone EQUtoN24--southern-most zone
# select data in this zone
test_zoneEQUtoN24 <- test_zone %>%
  dplyr::filter(c(zone == "EQUtoN24")) %>%
  dplyr::select(-c(zone))
# create prediction function--takes 1000 draws from poisson distribution
hurri_prediction_zoneEQUtoN24 <- function(X1){
  pred <- rpois(1000, exp(freq_sample3$beta0 + freq_sample3$beta1*X1 + freq_sample3$beta2.4.))
  return(pred)
}
# create empty vector to store predictions
hurri_preds_zoneEQUtoN24 <- rep(0, nrow(test_zoneEQUtoN24))
# get predictions--mean predicted value for each year in this zone
for (i in 1:nrow(test_zoneEQUtoN24)){
  hurri_preds_zoneEQUtoN24[i] <- mean(hurri_prediction_zoneEQUtoN24(X1 = test_zoneEQUtoN24$temp[i]))
}

# Compare results to actual data
zoneEQUtoN24 <- cbind(preds = as.matrix(hurri_preds_zoneEQUtoN24), test_zoneEQUtoN24)
zoneEQUtoN24
ggplot(zoneEQUtoN24, aes(x = year)) + geom_point(aes(y = preds, color = "predicted")) + geom_point(aes(y = total, color = "actual")) + xlim(1996, 2008) + ylim(0, 400) + ylab("total hurricanes") + ggtitle("0N to 24N")

zoneN24to44 <- cbind(preds = as.matrix(hurri_preds_zoneN24to44), test_zoneN24to44)
ggplot(zoneN24to44, aes(x = year)) + geom_point(aes(y = preds, color = "predicted")) + geom_point(aes(y = total, color = "actual")) + xlim(1996, 2008) + ylim(0, 400)+ ylab("total hurricanes") + ggtitle("24N to 44N")

zoneN44to64 <- cbind(preds = as.matrix(hurri_preds_zoneN44to64), test_zoneN44to64)
ggplot(zoneN44to64, aes(x = year)) + geom_point(aes(y = preds, color = "predicted")) + geom_point(aes(y = total, color = "actual")) + xlim(1996, 2008) + ylim(0, 400)+ ylab("total hurricanes") + ggtitle("44N to 64N")

zoneN64to90 <- cbind(preds = as.matrix(hurri_preds_zoneN64to90), test_zoneN64to90)
ggplot(zoneN64to90, aes(x = year)) + geom_point(aes(y = preds, color = "predicted"), position = "jitter") + geom_point(aes(y = total, color = "actual")) + xlim(1996, 2008) + ylim(0, 400)+ ylab("total hurricanes") + ggtitle("64N to 90N")


# find mean squared error
all_predictions <- rbind(zoneEQUtoN24, zoneN24to44, zoneN44to64, zoneN64to90)

View(all_predictions)

sum_diff <- 0
for (i in 1:nrow(all_predictions)) {
  sq_diff <- (all_predictions$total[i] - all_predictions$preds[i])^2
  sum_diff <- sum_diff + sq_diff
  print(sq_diff)
}

sum_diff/nrow(all_predictions)
```









=======
duration_jags <- jags.model(textConnection(duration_model), data=list(y = durations$duration, X1 = durations$temp, X2 = durations$zone), inits=list(.RNG.name="base::Wichmann-Hill", .RNG.seed=1989))



duration_sim <- coda.samples(duration_jags, variable.names=c("beta0","beta1", "beta2","tau"), n.iter=10000)


   
duration_samples <- data.frame(duration_sim[[1]])
head(duration_samples)
plot(duration_sim)
```

```{r}
running_mean_plot(x=duration_samples$beta0, se=TRUE)
running_mean_plot(x=duration_samples$beta1, se=TRUE)
>>>>>>> 9253d0d5ee157444cd7fe9184841d0ec9949ff8c

running_mean_plot(x=duration_samples$beta2.2., se=TRUE)
running_mean_plot(x=duration_samples$beta2.3., se=TRUE)
running_mean_plot(x=duration_samples$beta2.4., se=TRUE)
running_mean_plot(x=duration_samples$tau, se=TRUE)
```


```{r}
quantile(duration_samples$beta0, c(0.05, 0.975)) 
quantile(duration_samples$beta1, c(0.05, 0.975))
quantile(duration_samples$beta2.2., c(0.05, 0.975))
quantile(duration_samples$beta2.3., c(0.05, 0.975))
quantile(duration_samples$beta2.4., c(0.05, 0.975))

mean(duration_samples$beta0)
mean(duration_samples$beta1)
mean(duration_samples$beta2.2.)
mean(duration_samples$beta2.3.)
mean(duration_samples$beta2.4.)
```