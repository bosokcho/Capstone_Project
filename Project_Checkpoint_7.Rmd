# Checkpoint 7

** Madeline Abbott, Aidan Teppema, Daisy Cho **

\


```{r include=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(zoo)
library(lubridate)
library(ggmap)
library(reshape2)
library(MASS)
library(viridis)
#library(shiny)
library(rjags)
library(MacBayes)
```

Loading the data...
```{r include=FALSE}
# Loading the Data
hurricane_data <- read_csv("Historical_Tropical_Storm_Tracks.csv")
hurricane_data<-na.omit(hurricane_data)
hurricane_data<-subset(hurricane_data, select=-c(BTID, AD_TIME))

# Recent Hurricanes--select only hurricanes in 1950 and after
recent_hurricanes <- hurricane_data[hurricane_data$YEAR >= 1950, ] 
recent_hurricanes <- recent_hurricanes[!(recent_hurricanes$NAME=="NOTNAMED"),]
#Format Coordinates
dat<-as.character(recent_hurricanes$LAT)
new<-substr(dat,1,nchar(dat))
lat<-as.numeric(new)
dat<-as.character(recent_hurricanes$LONG)
new<-substr(dat,1,nchar(dat))
long<-as.numeric(new)
long<- -abs(long)
coord<-as.data.frame(long)
#Add year and name
year<-as.character(recent_hurricanes$YEAR)
name<-as.character(recent_hurricanes$NAME)
hurricane_locations<-cbind(year, name, coord, lat)

# Hurricane Frequency
# table of hurricanes per year
hurricanes_per_year <- recent_hurricanes %>%
  group_by(YEAR) %>%
  summarise(TOTAL_H = n_distinct(NAME))
hurricanes_per_basin <- recent_hurricanes %>%
  group_by(BASIN) %>%
  summarise(TOTAL_H = n_distinct(NAME, DAY))

# Temperature Data
temp_data <- read.csv("ZonAnn.csv")
temp_data[!complete.cases(temp_data),]
temp_data<-na.omit(temp_data)
temp_data<-temp_data[,-(5:15),drop=FALSE]
temp_recent <- temp_data %>%
  filter(Year > 1949)
# select only northern hemisphere temp data
tempN <- subset(temp_recent, select = c(Year, NHem))
# rename the columns
colnames(tempN) <- c("YEAR", "temp")
# join temperature to hurricanes per year table
hurricanes_per_year2 <- merge(x = hurricanes_per_year, y = tempN, by = "YEAR", all.x = TRUE)

# CO2 Data
CO2 <- read_csv("CO2.csv")
# Alter CO2 to remove unnecessary variable trend
CO2 <- subset(CO2, select = -trend)
# get average by year
CO2_yr <- CO2 %>%
  group_by(year) %>%
  summarise(avg_CO2 = mean(average))
# rename columns
colnames(CO2_yr) <- c("YEAR", "avg_CO2")
# join with hurricane per year table
hurricanes_per_year3 <- merge(x = hurricanes_per_year2, y = CO2_yr, by = "YEAR", all.x = TRUE)
# FIX THIS (bc only 1980 onward available)

# NAO Index Data
NAO_index1 <- read_csv("NAO_index_monthly.csv", 
    col_types = cols(Apr = col_number(), 
        Aug = col_number(), Dec = col_number(), 
        Feb = col_number(), Jan = col_number(), 
        Jul = col_number(), Jun = col_number(), 
        Mar = col_number(), May = col_number(), 
        Nov = col_number(), Oct = col_number(), 
        Sep = col_number()))
NAO_index2 <- NAO_index1[, -c(14)]
NAO_index3 <- NAO_index2 %>%
  rename(year = X1) %>%
  mutate(Jul = replace(Jul, Jul < -50| Jul > 50, "")) %>%
  mutate(Aug = replace(Aug, Aug < -50| Aug > 50, "")) %>%
  mutate(Sep = replace(Sep, Sep < -50| Sep > 50, "")) %>%
  mutate(Oct = replace(Oct, Oct < -50| Oct > 50, "")) %>%
  mutate(Nov = replace(Nov, Nov < -50| Nov > 50, "")) %>%
  mutate(Dec = replace(Dec, Dec < -50| Dec > 50, ""))
NAO_index <- NAO_index3[-nrow(NAO_index3),] 
# remove 2017 (data for year is incomplete)
NAO_index <- head(NAO_index, -1) 
# reshape table
NAO_index <- melt(NAO_index, id=c("year"))
colnames(NAO_index)[2] <- "month"
colnames(NAO_index)[3] <- "index"
NAO_index$year <- as.factor(NAO_index$year)
NAO_index$index <- as.numeric(NAO_index$index)
# group by year
NAO_yr <- NAO_index %>%
  group_by(year) %>%
  summarise(avg_NAO = mean(index))
# rename columns
colnames(NAO_yr) <- c("YEAR", "avg_NAO")
# join with hurricane data
hurricanes_per_year4 <- merge(x = hurricanes_per_year3, y = NAO_yr, by = "YEAR", all.x = TRUE)

# Remove all rows with missing data
hurricanes_per_year_clean<-na.omit(hurricanes_per_year4)

head(hurricanes_per_year_clean)
```


# Progress Made:

Possible things we could do:

* include only Atlantic ocean hurricanes [ ]

* check for colinearity between explanatory variables [DONE]

* include year as a predictor in model [ ]

* model for hurricane severity (see Alicia's email) [ ]

* predict occurance of next (severe landfall?) hurricane using exponential distribution [ ]

\
\


# Group Member Roles:

* Madeline--
* Daisy--
* Aidan--

\
\


# Modeling:


\


# Creating Testing and Training Data
```{r}
# use hurricanes per year data
rownames(hurricanes_per_year_clean) <- 1:nrow(hurricanes_per_year_clean)

# select first 80% of data
train_ind <- rep(1:(0.80*nrow(hurricanes_per_year_clean)))
hurricanes_per_year_train <- hurricanes_per_year_clean[train_ind, ]

# use the remaining 20% of the data for testing
hurricanes_per_year_test <- hurricanes_per_year_clean[-train_ind, ]
```


## Question 1.

Can we model the change in frequency of hurricanes based on trends in hurricane windspeed and category, temperature, CO2, and NAO index?


First, we check for colinarity between the explanatory variables of temperature, CO2, and NAO index.
```{r}
cor(x = as.matrix(hurricanes_per_year_clean$temp), y = as.matrix(hurricanes_per_year_clean$avg_CO2))

cor(x = as.matrix(hurricanes_per_year_clean$avg_NAO), y = as.matrix(hurricanes_per_year_clean$temp))

cor(x = as.matrix(hurricanes_per_year_clean$avg_NAO), y = as.matrix(hurricanes_per_year_clean$avg_CO2))
```

Based on the correlation coefficient between temperature and average CO2 concentration, we remove temperature to reduce collinarity.

Build a new poisson regression model for predicting using only NAO index and CO2 concentration.

Let:

$Y_{t} =$ total hurricanes in year $t$

$X_{1t} =$ yearly CO2 concentration in year $t$

$X_{2t} =$ NAO index in year $t$

We propose the following model:

$$Y_t | \beta_{0}, \beta_{1}, \beta_{2} \sim Pois(\lambda_i)$$

$$log(\lambda_i) = \beta_0 + \beta_{1}X_{1t} + \beta_{2}X_{2t}$$

$$\beta_0 \sim N(0, 1e6)$$

$$\beta_{1} \sim N(0, 1e6)$$

$$\beta_{2} \sim N(0, 1e6)$$



Simulating the model using rjags:
```{r}
head(hurricanes_per_year_train)

hur_mod <- " model {
  for (i in 1:length(TOTAL_H)) {
      TOTAL_H[i] ~ dpois(lam[i])
      log(lam[i]) = beta0 + beta1*X1[i] + beta2*X2[i]
  }

  beta0 ~ dnorm(0.0, 1.0/1e6)
  beta1 ~ dnorm(0.0, 1.0/1e4)
  beta2 ~ dnorm(0.0, 1.0/1e4)
} "

# set up an algorithm to simulate the posterior by combining the model and data (x)
# set the random number seed

data_jags = as.list(hurricanes_per_year_clean[,2:5])
str(data_jags)

# manually initialize parameters
freq_jags <- jags.model(textConnection(hur_mod),data=list(TOTAL_H = hurricanes_per_year_train$TOTAL_H, X1 = hurricanes_per_year_train$avg_CO2,X2 = hurricanes_per_year_train$avg_NAO), inits=list(beta0=5,beta1=0, beta2=0))

# simulate a sample from the posterior
# note that we specify both mu and tau variables
freq_sim <- coda.samples(freq_jags, variable.names = c("beta0", "beta1", "beta2"), n.iter=500000)

# store the samples in a data frame:
freq_sample <- data.frame(step = 1:500000, freq_sim[[1]])
head(freq_sample, 10)


# make a dataframe of parameter values every other step to make plotting faster
Nth.delete<-function(dataframe, n)dataframe[-(seq(n,to=nrow(dataframe),by=n)),]
freq_sample_small <- Nth.delete(freq_sample, 2)
```

Looking at convergence
```{r}
library(MacBayes)
#running_mean_plot(x=freq_sample_small$beta0, se=TRUE)
#running_mean_plot(x=freq_sample_small$beta1, se=TRUE)
#running_mean_plot(x=freq_sample_small$beta2, se=TRUE)
```

Based on the running mean plots, parameters look like they have converged.  Parameter mean values and 95% credible intervals are as follows:
```{r}
quantile(freq_sample$beta0, c(0.05, 0.975)) 
quantile(freq_sample$beta1, c(0.05, 0.975))
quantile(freq_sample$beta2, c(0.05, 0.975))

mean(freq_sample$beta0)
mean(freq_sample$beta1)
mean(freq_sample$beta2)
```

Beta0 has a mean of 3.543205 and a 95% CI of (1.209219, 6.417573.

Beta1 has a mean of -0.000656804 and a 95% CI of (-0.007524760,  0.007268657).

Beta2 has a mean of 0.03147051 and a 95% CI of (-0.0852778,  0.1710288).

\

From these results, we conclude that average annual CO2 concentration and NAO index are not useful predictors of hurricane frequency.
FIX THIS
```{r}
#log(lam[i]) = beta0 + beta1*X1[i] + beta2*X2[i]

predictions <- rep(0, nrow(hurricanes_per_year_test)) 
for (i in 1:nrow(hurricanes_per_year_test)){
  predictions[i] <- rpois(1, lambda=exp(freq_sample$beta0 + freq_sample$beta1*hurricanes_per_year_test$avg_CO2[i] + freq_sample$beta2*hurricanes_per_year_test$avg_NAO[i]))
}
```





## Hurricanes by Zones

Divide northern hemisphere into four zones.  Get temperature from 1950 for each of these zones.
```{r}
temp_data <- read_csv("~/Desktop/ZonAnn.csv")
temp_data<-na.omit(temp_data) %>%
  filter(1950 <= Year) %>%
  filter(Year <= 2008)
my_temp <- subset(temp_data, select = c("Year", "64N-90N", "44N-64N", "24N-44N", "EQU-24N"))
colnames(my_temp) <- c("Year", "N64to90", "N44to64", "N24to44", "EQUtoN24")
my_temp <- melt(my_temp, id=c("Year"))
colnames(my_temp) <- c("year", "zone", "temp")
head(my_temp)
```

Now divide hurricane observations into these same four zones.
(Currently using all hurricane locations to see if temperature affects where hurricanes occur, not just where they are the most severe.  Could also switch to use just the location at which each hurricane is most severe.)
```{r}
head(hurricane_locations)

hurricanes_zones <- hurricane_locations %>% 
  mutate(zone=cut(lat, breaks=c(0, 24, 44, 64, 90), labels=c("EQUtoN24", "N24to44", "N44to64", "N64to90"))) %>%
  group_by(year, zone)  %>%
  summarise(total = n())

head(hurricanes_zones, 40)
```

Combine the hurricane and temperature data frames together
```{r}
hurricanes_per_zone <- merge(my_temp, hurricanes_zones, by = c("year", "zone"), all.x = TRUE, all.y = TRUE)
hurricanes_per_zone[is.na(hurricanes_per_zone)] <- 0
head(hurricanes_per_zone)
```


Poisson regression--just with temperature
```{r}
hur_mod2 <- " model {
  for (i in 1:length(total)) {
      total[i] ~ dpois(lam[i])
      log(lam[i]) = beta0 + beta1*X1[i]
  }

  beta0 ~ dnorm(0.0, 1.0/1e6)
  beta1 ~ dnorm(0.0, 1.0/1e4)
} "

# set up an algorithm to simulate the posterior by combining the model and data (x)
# set the random number seed
freq_jags2 <- jags.model(textConnection(hur_mod2),data=list(total = hurricanes_per_zone$total, X1 = hurricanes_per_zone$temp), inits=list(.RNG.name="base::Wichmann-Hill", .RNG.seed=2000))

# simulate a sample from the posterior
freq_sim2 <- coda.samples(freq_jags2, variable.names = c("beta0", "beta1"), n.iter=5000)

# store the samples in a data frame:
freq_sample2 <- data.frame(step = 1:5000, freq_sim2[[1]])
head(freq_sample2, 10)
```

Look at the estimated parameters--have they coverged?
```{r}
running_mean_plot(x=freq_sample2$beta0, se=TRUE)
running_mean_plot(x=freq_sample2$beta1, se=TRUE)
```

Based on the running mean plots, parameters look like they have converged.  Parameter mean values and 95% credible intervals are as follows:
```{r}
quantile(freq_sample2$beta0, c(0.05, 0.975)) 
quantile(freq_sample2$beta1, c(0.05, 0.975))

mean(freq_sample2$beta0)
mean(freq_sample2$beta1)
```

beta1, x1 is temp; beta2, x2 is zone

Look at the data
```{r}
head(hurricanes_per_zone)
train_zone <- hurricanes_per_zone %>%
  filter(year < 1996)
test_zone <- hurricanes_per_zone %>%
  filter(year >= 1996)
```



Poisson regression--with temperature and location zone
```{r}
hur_mod3 <- " model {
  for (i in 1:length(total)) {
      total[i] ~ dpois(lam[i])
      log(lam[i]) = beta0 + beta1*X1[i] + beta2[X2[i]]
  }

  beta0 ~ dnorm(0.0, 1.0/1e4)
  beta1 ~ dnorm(0.0, 1.0/1e4)
  beta2[1] <- 0
  for (i in 2:4) {
    beta2[i] ~ dnorm(0.0, 1.0/1e4)
  }
} "

# set up an algorithm to simulate the posterior by combining the model and data (x)
# set the random number seed
freq_jags3 <- jags.model(textConnection(hur_mod3),data=list(total = train_zone$total, X1 = train_zone$temp, X2 = train_zone$zone), inits=list(.RNG.name="base::Wichmann-Hill", .RNG.seed=2000))

# simulate a sample from the posterior
freq_sim3 <- coda.samples(freq_jags3, variable.names = c("beta0", "beta1", "beta2"), n.iter=1000)

# store the samples in a data frame:
freq_sample3 <- data.frame(step = 1:1000, freq_sim3[[1]])
head(freq_sample3, 10)
plot(freq_sim3)
```



Check for convergence
```{r}
running_mean_plot(x=freq_sample3$beta0, se=TRUE)
running_mean_plot(x=freq_sample3$beta1, se=TRUE)

running_mean_plot(x=freq_sample3$beta2.1, se=TRUE)
running_mean_plot(x=freq_sample3$beta2.2., se=TRUE)
running_mean_plot(x=freq_sample3$beta2.3., se=TRUE)
running_mean_plot(x=freq_sample3$beta2.4., se=TRUE)
```



Look at parameter values
```{r}
# beta 0
quantile(freq_sample3$beta0, c(0.05, 0.975))
# beta 1
quantile(freq_sample3$beta1, c(0.05, 0.975))
# beta 2.2
quantile(freq_sample3$beta2.2., c(0.05, 0.975))
# beta 2.3
quantile(freq_sample3$beta2.3., c(0.05, 0.975))
# beta 2.4
quantile(freq_sample3$beta2.4., c(0.05, 0.975))

# beta 0
mean(freq_sample3$beta0)
# beta 1
mean(freq_sample3$beta1)
# beta 2.2
mean(freq_sample3$beta2.2.)
# beta 2.3
mean(freq_sample3$beta2.3.)
# beta 2.4
mean(freq_sample3$beta2.4.)

summary(freq_sample3)
```


Testing the model
```{r}
library(dplyr)

# Testing Zone N64to90--begin with the northern-most zone
# select data in this zone
test_zoneN64to90 <- test_zone %>%
  dplyr::filter(c(zone == "N64to90")) %>%
  dplyr::select(-c(zone))
# create prediction function--takes 1000 draws from poisson distribution
hurri_prediction_zoneN64to90 <- function(X1){
  pred <- rpois(1000, freq_sample3$beta0 + freq_sample3$beta2.4. + freq_sample3$beta1*X1)
  return(pred)
}
# create empty vector to store predictions
hurri_preds_zoneN64to90 <- rep(0, nrow(test_zoneN64to90))
# get predictions--mean predicted value for each year in this zone
# BUT NAs ARE PRODUCED...
for (i in 1:nrow(test_zoneN64to90)){
  hurri_preds_zoneN64to90[i] <- mean(hurri_prediction_zoneN64to90(X1 = test_zoneN64to90$temp[i]))
}


# Testing Zone N44to64--next northern-most zone
# select data in this zone
test_zoneN44to64 <- test_zone %>%
  dplyr::filter(c(zone == "N44to64")) %>%
  dplyr::select(-c(zone))
# create prediction function--takes 1000 draws from poisson distribution
hurri_prediction_zoneN44to64 <- function(X1){
  pred <- rpois(1000, freq_sample3$beta0 + freq_sample3$beta2.3. + freq_sample3$beta1*X1)
  return(pred)
}
# create empty vector to store predictions
hurri_preds_zoneN44to64 <- rep(0, nrow(test_zoneN44to64))
# get predictions--mean predicted value for each year in this zone
# BUT NAs ARE PRODUCED...
for (i in 1:nrow(test_zoneN44to64)){
  hurri_preds_zoneN44to64[i] <- mean(hurri_prediction_zoneN44to64(X1 = test_zoneN44to64$temp[i]))
}


# Testing Zone N24to44--the next northern-most zone

# select data in this zone
test_zoneN24to44 <- test_zone %>%
  dplyr::filter(c(zone == "N24to44")) %>%
  dplyr::select(-c(zone))
# create prediction function--takes 1000 draws from poisson distribution
hurri_prediction_zoneN24to44 <- function(X1){
  pred <- rpois(1000, freq_sample3$beta0 + freq_sample3$beta2.2. + freq_sample3$beta1*X1)
  return(pred)
}
# create empty vector to store predictions
hurri_preds_zoneN24to44 <- rep(0, nrow(test_zoneN24to44))
# get predictions--mean predicted value for each year in this zone
# BUT NAs ARE PRODUCED...
for (i in 1:nrow(test_zoneN24to44)){
  hurri_preds_zoneN24to44[i] <- mean(hurri_prediction_zoneN24to44(X1 = test_zoneN24to44$temp[i]))
}


# Testing Zone EQUtoN24--southern-most zone

# select data in this zone
test_zoneEQUtoN24 <- test_zone %>%
  dplyr::filter(c(zone == "EQUtoN24")) %>%
  dplyr::select(-c(zone))
# create prediction function--takes 1000 draws from poisson distribution
hurri_prediction_zoneEQUtoN24 <- function(X1){
  pred <- rpois(1000, exp(freq_sample3$beta0 + freq_sample3$beta1*X1))
  return(pred)
}
# create empty vector to store predictions
hurri_preds_zoneEQUtoN24 <- rep(0, nrow(test_zoneEQUtoN24))
# get predictions--mean predicted value for each year in this zone
# BUT NAs ARE PRODUCED...
for (i in 1:nrow(test_zoneEQUtoN24)){
  hurri_preds_zoneEQUtoN24[i] <- mean(hurri_prediction_zoneEQUtoN24(X1 = test_zoneEQUtoN24$temp[i]))
}

# Do the same for each of the other zones and compare results to actual data
```












