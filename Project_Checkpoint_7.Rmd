# Checkpoint 7

** Madeline Abbott, Aidan Teppema, Daisy Cho **

\


```{r include=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(zoo)
library(lubridate)
library(ggmap)
library(reshape2)
library(MASS)
library(viridis)
#library(shiny)
library(rjags)
library(MacBayes)
```

Loading the data...
```{r include=FALSE}
# Loading the Data
hurricane_data <- read_csv("Historical_Tropical_Storm_Tracks.csv")
hurricane_data<-na.omit(hurricane_data)
hurricane_data<-subset(hurricane_data, select=-c(BTID, AD_TIME))

# Recent Hurricanes--select only hurricanes in 1950 and after
recent_hurricanes <- hurricane_data[hurricane_data$YEAR >= 1950, ] 
recent_hurricanes <- recent_hurricanes[!(recent_hurricanes$NAME=="NOTNAMED"),]
#Format Coordinates
dat<-as.character(recent_hurricanes$LAT)
new<-substr(dat,1,nchar(dat))
lat<-as.numeric(new)
dat<-as.character(recent_hurricanes$LONG)
new<-substr(dat,1,nchar(dat))
long<-as.numeric(new)
long<- -abs(long)
coord<-as.data.frame(long)
#Add year and name
year<-as.character(recent_hurricanes$YEAR)
name<-as.character(recent_hurricanes$NAME)
hurricane_locations<-cbind(year, name, coord, lat)

# Hurricane Frequency
# table of hurricanes per year
hurricanes_per_year <- recent_hurricanes %>%
  group_by(YEAR) %>%
  summarise(TOTAL_H = n_distinct(NAME))
hurricanes_per_basin <- recent_hurricanes %>%
  group_by(BASIN) %>%
  summarise(TOTAL_H = n_distinct(NAME, DAY))

# Temperature Data
temp_data <- read.csv("ZonAnn.csv")
temp_data[!complete.cases(temp_data),]
temp_data<-na.omit(temp_data)
temp_data<-temp_data[,-(5:15),drop=FALSE]
temp_recent <- temp_data %>%
  filter(Year > 1949)
# select only northern hemisphere temp data
tempN <- subset(temp_recent, select = c(Year, NHem))
# rename the columns
colnames(tempN) <- c("YEAR", "temp")
# join temperature to hurricanes per year table
hurricanes_per_year2 <- merge(x = hurricanes_per_year, y = tempN, by = "YEAR", all.x = TRUE)

# CO2 Data
CO2 <- read_csv("CO2.csv")
# Alter CO2 to remove unnecessary variable trend
CO2 <- subset(CO2, select = -trend)
# get average by year
CO2_yr <- CO2 %>%
  group_by(year) %>%
  summarise(avg_CO2 = mean(average))
# rename columns
colnames(CO2_yr) <- c("YEAR", "avg_CO2")
# join with hurricane per year table
hurricanes_per_year3 <- merge(x = hurricanes_per_year2, y = CO2_yr, by = "YEAR", all.x = TRUE)
# FIX THIS (bc only 1980 onward available)

# NAO Index Data
NAO_index1 <- read_csv("NAO_index_monthly.csv", 
    col_types = cols(Apr = col_number(), 
        Aug = col_number(), Dec = col_number(), 
        Feb = col_number(), Jan = col_number(), 
        Jul = col_number(), Jun = col_number(), 
        Mar = col_number(), May = col_number(), 
        Nov = col_number(), Oct = col_number(), 
        Sep = col_number()))
NAO_index2 <- NAO_index1[, -c(14)]
NAO_index3 <- NAO_index2 %>%
  rename(year = X1) %>%
  mutate(Jul = replace(Jul, Jul < -50| Jul > 50, "")) %>%
  mutate(Aug = replace(Aug, Aug < -50| Aug > 50, "")) %>%
  mutate(Sep = replace(Sep, Sep < -50| Sep > 50, "")) %>%
  mutate(Oct = replace(Oct, Oct < -50| Oct > 50, "")) %>%
  mutate(Nov = replace(Nov, Nov < -50| Nov > 50, "")) %>%
  mutate(Dec = replace(Dec, Dec < -50| Dec > 50, ""))
NAO_index <- NAO_index3[-nrow(NAO_index3),] 
# remove 2017 (data for year is incomplete)
NAO_index <- head(NAO_index, -1) 
# reshape table
NAO_index <- melt(NAO_index, id=c("year"))
colnames(NAO_index)[2] <- "month"
colnames(NAO_index)[3] <- "index"
NAO_index$year <- as.factor(NAO_index$year)
NAO_index$index <- as.numeric(NAO_index$index)
# group by year
NAO_yr <- NAO_index %>%
  group_by(year) %>%
  summarise(avg_NAO = mean(index))
# rename columns
colnames(NAO_yr) <- c("YEAR", "avg_NAO")
# join with hurricane data
hurricanes_per_year4 <- merge(x = hurricanes_per_year3, y = NAO_yr, by = "YEAR", all.x = TRUE)

# Remove all rows with missing data
hurricanes_per_year_clean<-na.omit(hurricanes_per_year4)

head(hurricanes_per_year_clean)
```


# Progress Made:

Possible things we could do:

* include only Atlantic ocean hurricanes [ ]

* check for colinearity between explanatory variables [DONE]

* include year as a predictor in model [ ]

* model for hurricane severity (see Alicia's email) [ ]

* predict occurance of next (severe landfall?) hurricane using exponential distribution [ ]

\
\


# Group Member Roles:

* Madeline--
* Daisy--
* Aidan--

\
\


# Modeling:


\


## Question 1.

Can we model the change in frequency of hurricanes based on trends in hurricane windspeed and category, temperature, CO2, and NAO index?


First, we check for colinarity between the explanatory variables of temperature, CO2, and NAO index.
```{r}
cor(x = as.matrix(hurricanes_per_year_clean$temp), y = as.matrix(hurricanes_per_year_clean$avg_CO2))

cor(x = as.matrix(hurricanes_per_year_clean$avg_NAO), y = as.matrix(hurricanes_per_year_clean$temp))

cor(x = as.matrix(hurricanes_per_year_clean$avg_NAO), y = as.matrix(hurricanes_per_year_clean$avg_CO2))
```

Based on the correlation coefficient between temperature and average CO2 concentration, we remove temperature to reduce collinarity.

Build a new poisson regression model for predicting using only NAO index and CO2 concentration.

Let:

$Y_{t} =$ total hurricanes in year $t$

$X_{1t} =$ yearly CO2 concentration in year $t$

$X_{2t} =$ NAO index in year $t$

We propose the following model:

$$Y_t | \beta_{0}, \beta_{1}, \beta_{2} \sim Pois(\lambda_i)$$

$$log(\lambda_i) = \beta_0 + \beta_{1}X_{1t} + \beta_{2}X_{2t}$$

$$\beta_0 \sim N(0, 1e6)$$

$$\beta_{1} \sim N(0, 1e6)$$

$$\beta_{2} \sim N(0, 1e6)$$



Simulating the model using rjags:
```{r}
head(hurricanes_per_year_clean)

hur_mod <- " model {
  for (i in 1:length(TOTAL_H)) {
      TOTAL_H[i] ~ dpois(lam[i])
      log(lam[i]) = beta0 + beta1*X1[i] + beta2*X2[i]
  }

  beta0 ~ dnorm(0.0, 1.0/1e6)
  beta1 ~ dnorm(0.0, 1.0/1e4)
  beta2 ~ dnorm(0.0, 1.0/1e4)
} "

# set up an algorithm to simulate the posterior by combining the model and data (x)
# set the random number seed

data_jags = as.list(hurricanes_per_year_clean[,2:5])
str(data_jags)

#freq_jags <- jags.model(textConnection(hur_mod),data=list(TOTAL_H = hurricanes_per_year_clean$TOTAL_H, X1 = hurricanes_per_year_clean$temp, X2 = hurricanes_per_year_clean$avg_CO2,X3 = hurricanes_per_year_clean$avg_NAO), inits=list(.RNG.name="base::Wichmann-Hill", .RNG.seed=2000))

# manually initialize parameters
freq_jags <- jags.model(textConnection(hur_mod),data=list(TOTAL_H = hurricanes_per_year_clean$TOTAL_H, X1 = hurricanes_per_year_clean$avg_CO2,X2 = hurricanes_per_year_clean$avg_NAO), inits=list(beta0=5,beta1=0, beta2=0))

# simulate a sample from the posterior
# note that we specify both mu and tau variables
freq_sim <- coda.samples(freq_jags, variable.names = c("beta0", "beta1", "beta2"), n.iter=500000)

# store the samples in a data frame:
freq_sample <- data.frame(step = 1:500000, freq_sim[[1]])
head(freq_sample, 10)


# make a dataframe of parameter values every other step to make plotting faster
Nth.delete<-function(dataframe, n)dataframe[-(seq(n,to=nrow(dataframe),by=n)),]
freq_sample_small <- Nth.delete(freq_sample, 2)
```

Looking at convergence
```{r}
library(MacBayes)
running_mean_plot(x=freq_sample_small$beta0, se=TRUE)
running_mean_plot(x=freq_sample_small$beta1, se=TRUE)
running_mean_plot(x=freq_sample_small$beta2, se=TRUE)
```

Based on the running mean plots, parameters look like they have converged.  Parameter mean values and 95% credible intervals are as follows:
```{r}
quantile(freq_sample$beta0, c(0.05, 0.975)) 
quantile(freq_sample$beta1, c(0.05, 0.975))
quantile(freq_sample$beta2, c(0.05, 0.975))

mean(freq_sample$beta0)
mean(freq_sample$beta1)
mean(freq_sample$beta2)
```

Beta0 has a mean of 2.327235 and a 95% CI of (0.7864641, 4.1668185).

Beta1 has a mean of 0.002818374 and a 95% CI of (-0.001344940, 0.007860139).

Beta2 has a mean of 0.001306308 and a 95% CI of (-0.1105324, 0.1350339).

\

From these results, we conclude that average annual CO2 concentration and NAO index are not useful predictors of hurricane frequency.


## Hurricanes by Zones

Divide northern hemisphere into four zones.  Get temperature from 1950 for each of these zones.
```{r}
temp_data <- read_csv("~/Desktop/ZonAnn.csv")
temp_data<-na.omit(temp_data) %>%
  filter(1950 <= Year) %>%
  filter(Year <= 2008)
my_temp <- subset(temp_data, select = c("Year", "64N-90N", "44N-64N", "24N-44N", "EQU-24N"))
colnames(my_temp) <- c("Year", "N64to90", "N44to64", "N24to44", "EQUtoN24")
my_temp <- melt(my_temp, id=c("Year"))
colnames(my_temp) <- c("year", "zone", "temp")
head(my_temp)
```

Now divide hurricane observations into these same four zones.
(Currently using all hurricane locations to see if temperature affects where hurricanes occur, not just where they are the most severe.  Could also switch to use just the location at which each hurricane is most severe.)
```{r}
head(hurricane_locations)

hurricanes_zones <- hurricane_locations %>% 
  mutate(zone=cut(lat, breaks=c(0, 24, 44, 64, 90), labels=c("N64to90", "N44to64", "N24to44", "EQUtoN24"))) %>%
  group_by(year, zone)  %>%
  summarise(total = n())

head(hurricanes_zones, 40)
```

Combine the hurricane and temperature data frames together
```{r}
hurricanes_per_zone <- merge(my_temp, hurricanes_zones, by = c("year", "zone"), all.x = TRUE, all.y = TRUE)
hurricanes_per_zone[is.na(hurricanes_per_zone)] <- 0
head(hurricanes_per_zone)
```


Poisson regression--just with temperature
```{r}
hur_mod2 <- " model {
  for (i in 1:length(total)) {
      total[i] ~ dpois(lam[i])
      log(lam[i]) = beta0 + beta1*X1[i]
  }

  beta0 ~ dnorm(0.0, 1.0/1e6)
  beta1 ~ dnorm(0.0, 1.0/1e4)
} "

# set up an algorithm to simulate the posterior by combining the model and data (x)
# set the random number seed
freq_jags2 <- jags.model(textConnection(hur_mod2),data=list(total = hurricanes_per_zone$total, X1 = hurricanes_per_zone$temp), inits=list(.RNG.name="base::Wichmann-Hill", .RNG.seed=2000))

# simulate a sample from the posterior
freq_sim2 <- coda.samples(freq_jags2, variable.names = c("beta0", "beta1"), n.iter=5000)

# store the samples in a data frame:
freq_sample2 <- data.frame(step = 1:5000, freq_sim2[[1]])
head(freq_sample2, 10)
```

Look at the estimated parameters--have they coverged?
```{r}
running_mean_plot(x=freq_sample2$beta0, se=TRUE)
running_mean_plot(x=freq_sample2$beta1, se=TRUE)
```

Based on the running mean plots, parameters look like they have converged.  Parameter mean values and 95% credible intervals are as follows:
```{r}
quantile(freq_sample2$beta0, c(0.05, 0.975)) 
quantile(freq_sample2$beta1, c(0.05, 0.975))

mean(freq_sample2$beta0)
mean(freq_sample2$beta1)
```

beta1, x1 is temp; beta2, x2 is zone


Poisson regression--with temperature and location zone
```{r}
hur_mod3 <- " model {
  for (i in 1:length(total)) {
      total[i] ~ dpois(lam[i])
      log(lam[i]) = beta0 + beta1*X1[i] + beta2[X2[i]]
  }

  beta0 ~ dnorm(0.0, 1.0/1e4)
  beta1 ~ dnorm(0.0, 1.0/1e4)
  beta2[1] <- 0
  for (i in 2:4) {
    beta2 ~ dnorm(0.0, 1.0/1e4)
  }
} "

# set up an algorithm to simulate the posterior by combining the model and data (x)
# set the random number seed
freq_jags3 <- jags.model(textConnection(hur_mod3),data=list(total = hurricanes_per_zone$total, X1 = hurricanes_per_zone$temp, X2 = hurricanes_per_zone$zone), inits=list(.RNG.name="base::Wichmann-Hill", .RNG.seed=2000))

# simulate a sample from the posterior
freq_sim3 <- coda.samples(freq_jags3, variable.names = c("beta0", "beta1", "beta2"), n.iter=1000)

# store the samples in a data frame:
freq_sample3 <- data.frame(step = 1:1000, freq_sim3[[1]])
head(freq_sample3, 10)
```

Check for convergence
```{r}
running_mean_plot(x=freq_sample3$beta0, se=TRUE)
running_mean_plot(x=freq_sample3$beta1, se=TRUE)

running_mean_plot(x=freq_sample3$beta2.2., se=TRUE)
running_mean_plot(x=freq_sample3$beta2.3., se=TRUE)
running_mean_plot(x=freq_sample3$beta2.4., se=TRUE)
```



Look at parameter values
```{r}
quantile(freq_sample3$beta0, c(0.05, 0.975)) 
quantile(freq_sample3$beta1, c(0.05, 0.975))
quantile(freq_sample3$beta2.2., c(0.05, 0.975))
quantile(freq_sample3$beta2.3., c(0.05, 0.975))
quantile(freq_sample3$beta2.4., c(0.05, 0.975))

mean(freq_sample3$beta0)
mean(freq_sample3$beta1)
mean(freq_sample3$beta2.2.)
mean(freq_sample3$beta2.3.)
mean(freq_sample3$beta2.4.)
```


















```{r}

library(maps)
library(mapdata)
library(maptools)
library(readr)
library(dplyr)
library(ggplot2)
library(rgdal)
library(sp)
library(dplyr)
hurricane_data <- read.csv("Historical_Tropical_Storm_Tracks.csv")
#area <- readShapePoly("ne_10m_coastline.shp")

hurricane_data<-na.omit(hurricane_data)
hurricane_data<-subset(hurricane_data, select=-c(FID, BTID, AD_TIME))

# select only hurricanes in 1950 and after
recent_hurricanes <- hurricane_data[hurricane_data$YEAR >= 1950, ] 
recent_hurricanes$Yearmonthday<-as.numeric(do.call(paste, c(recent_hurricanes[c("YEAR", "MONTH","DAY")], sep = ""))) #Creating a Yearmonth columns which will be useful for identifying the landfall date


global<-map_data("world")

location_max <- hurricane_data %>%
  filter(YEAR > 1949) %>%
  group_by(YEAR, NAME) 
global[1:2]<-round(global[1:2],1)
landfall <-merge(location_max, global, by.x = c("LONG","LAT"), by.y = c("long","lat"))
landfall<-subset(landfall, select =-c(group,order,region,subregion))
head(landfall)
library(rworldmap)
newmap <- getMap(resolution = "low")
plot(newmap)
points(landfall$LONG, landfall$LAT, col = "red", cex = .6)
```
```{r}
require(maps)
require(sp)
library(rgeos)
library(gstat)
dat<-as.character(recent_hurricanes$LAT)
new<-substr(dat,1,nchar(dat))
lat<-as.numeric(new)
dat<-as.character(recent_hurricanes$LONG)
new<-substr(dat,1,nchar(dat))
long<-as.numeric(new)
long<- -abs(long)
coord<-as.data.frame(long)
coord<-cbind(coord, lat)

global<-map("world", fill=TRUE)
IDs <- sapply(strsplit(global$names, ":"), function(x) x[1])
global <- map2SpatialPolygons(global, IDs=IDs, proj4string=CRS("+proj=longlat +datum=WGS84"))

# First I need to dissolve and create a unique polygon, then I'll create the coastal boundary
crs  <- CRS("+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-110 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m") 
global.p<-spTransform(global, crs)
global.p<-gBuffer(global.p, byid=TRUE, width=1) 
global.u<- gUnaryUnion(global.p)
global.l<-as(global,'SpatialLines') 
crs.new<-CRS(proj4string(global))
global.l<-spTransform(global.l, crs.new)
```

```{r}
recent_hurricanes <- recent_hurricanes %>%
  mutate(ID = group_indices_(recent_hurricanes, .dots=c("YEAR", "NAME"))) 
head(recent_hurricanes)

hu.ID<-unique(recent_hurricanes$ID) # Identifying the numbers of hurricanes on the dataset
m.data<-NULL # Initiate a NULL matrix where to store the results recursively 

options(warn=-1) # Removing the warning messages coming from the krige function
for (i in 1:length(hu.ID)) {
  
  id.storm<-hu.ID[i]
  storm.i<-recent_hurricanes[recent_hurricanes$ID==id.storm,]
    
  # Transform the Hurricane track into a line
  storm.c<- data.frame(x=storm.i$LONG,y=storm.i$LAT)
  coordinates(storm.c)<- ~x+y 
  proj4string(storm.c) <- CRS(proj4string(global))
  storm.l <- SpatialLines(list(Lines(list(Line(coordinates(storm.c))),"X")))
  proj4string(storm.l) <- CRS(proj4string(global)) ## attributing the coordinate system of the map (WGS84) to the ike track
    
  # Finding the landfall point of the hurricane (intersection between the track and the US coastline)
  cross<-gIntersection(storm.l,global.l)

  
  # If there is intersection, identify the wind speed, date (year and month) and State at landfall
  
  if (length(cross)>0)
  {
    storm.int<-data.frame(x=storm.i$LONG,y=storm.i$LAT,tim=storm.i$Yearmonthday)
    colnames(storm.int)<-c("Long","Lat","Yearmonth")
  
    # Convert the event track info into a spatial data frame
    coordinates(storm.int) <- ~Long+Lat
    proj4string(storm.int) <- CRS(proj4string(global))
    
    # Compute the interpolated values of the wind speed at landfall
    #landfall.k <- krige(Max_Speed~1, storm.int, cross, debug.level = 0)
    #landfall.w<-landfall.k$var1.pred
  
    # Compute the interpolated values of the Year month at landfall
    landfall.kt <- krige(Yearmonth~1, storm.int, cross, debug.level = 0)
    landfall.t<-landfall.kt$var1.pred
    
    landfall.p<-cbind(cross@coords,landfall.t)
    colnames(landfall.p)[3]<- ("Time")
    
    # Multiple landfalls can occure (Hurricane eye can intersect the coastline several times)
    # I will consider only one landfall per storm and pick the landfall point with the maximum wind
    lfmax.p<-landfall.p[which.max(landfall.p[,3]),]
    lfmax.p<-t(as.matrix(lfmax.p))
    lfmax.p[,c("Time")]<-round(lfmax.p[,c("Time")]) ## rounding the time in case the landfall point happens between 2 months
    
    # Identifying the state where landfall occurs
    lfmax.p1<-data.frame(x=lfmax.p[1], y=lfmax.p[2], wmax=lfmax.p[3])
    coordinates(lfmax.p1)<-~x+y
    proj4string(lfmax.p1) <- CRS(proj4string(global))
    crs  <- CRS("+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-110 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m") 
    lfmax.p1_pla<-spTransform(lfmax.p1, crs)
    usa_pla<-spTransform(global, crs)
    dist<-gDistance(lfmax.p1_pla,usa_pla, byid=TRUE)
    state.name<-rownames(dist)[which.min(dist)]
     
    # storm info to append
    st.info<-storm.i[1,c("ID","Name")]
    st.info<-cbind(st.info,state.name,lfmax.p)
    st.info$Year<-substr(st.info$Time,1,4)
    st.info$Month<-substr(st.info$Time,5,6)
    st.info<-st.info[,-which(colnames(st.info)=="Time")]
  
    # Append storm info
    m.data<-rbind(m.data,st.info)
    }
} 
options(warn=0)
```








